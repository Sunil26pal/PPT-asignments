{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aUNyv0pKwH80"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0OEOTn4uSVt",
        "outputId": "a449b5eb-88c3-488c-a298-2214d2156e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No core-site section found in the configuration file.\n"
          ]
        }
      ],
      "source": [
        "import configparser\n",
        "\n",
        "def read_hadoop_config(config_file):\n",
        "    # Create a ConfigParser object\n",
        "    config = configparser.ConfigParser()\n",
        "\n",
        "    # Read the Hadoop configuration file\n",
        "    config.read(config_file)\n",
        "\n",
        "    # Get the core section from the configuration file\n",
        "    if 'core-site' in config:\n",
        "        core_section = config['core-site']\n",
        "        print(\"Core Components of Hadoop:\")\n",
        "        for key in core_section:\n",
        "            print(key)\n",
        "    else:\n",
        "        print(\"No core-site section found in the configuration file.\")\n",
        "\n",
        "# Provide the path to your Hadoop configuration file\n",
        "config_file_path = '/path/to/hadoop-config-file.xml'\n",
        "\n",
        "# Call the function to read and display the core components\n",
        "read_hadoop_config(config_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n"
      ],
      "metadata": {
        "id": "BI6q195EwM1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def calculate_directory_size(hdfs_url, directory_path):\n",
        "    # Create an HDFS client\n",
        "    client = InsecureClient(hdfs_url)\n",
        "\n",
        "    # Get the file status of the directory\n",
        "    dir_status = client.status(directory_path)\n",
        "\n",
        "    # Check if the path is a directory\n",
        "    if not dir_status['type'] == 'DIRECTORY':\n",
        "        print(\"Error: The specified path is not a directory.\")\n",
        "        return\n",
        "\n",
        "    # Initialize the total size to 0\n",
        "    total_size = 0\n",
        "\n",
        "    # Recursively calculate the size of each file in the directory\n",
        "    def calculate_file_sizes(directory):\n",
        "        nonlocal total_size\n",
        "        for item in client.list(directory):\n",
        "            item_path = directory + '/' + item['name']\n",
        "            item_status = client.status(item_path)\n",
        "            if item_status['type'] == 'DIRECTORY':\n",
        "                calculate_file_sizes(item_path)\n",
        "            else:\n",
        "                total_size += item_status['length']\n",
        "\n",
        "    # Call the recursive function to calculate file sizes\n",
        "    calculate_file_sizes(directory_path)\n",
        "\n",
        "    # Print the total file size\n",
        "    print(\"Total file size in directory:\", total_size, \"bytes\")\n",
        "\n",
        "# Provide the HDFS URL and directory path\n",
        "hdfs_url = 'http://localhost:50070'\n",
        "directory_path = '/path/to/hdfs/directory'\n",
        "\n",
        "# Call the function to calculate the directory size\n",
        "calculate_directory_size(hdfs_url, directory_path)\n"
      ],
      "metadata": {
        "id": "D41GTXTLwRt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n"
      ],
      "metadata": {
        "id": "7Tz-2Y-MwRfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import re\n",
        "\n",
        "WORD_REGEX = re.compile(r\"[\\w']+\")\n",
        "\n",
        "class TopNWords(MRJob):\n",
        "\n",
        "    def configure_args(self):\n",
        "        super(TopNWords, self).configure_args()\n",
        "        self.add_passthru_arg('--top', type=int, help='Number of top words to display')\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_get_words,\n",
        "                   combiner=self.combiner_count_words,\n",
        "                   reducer=self.reducer_count_words),\n",
        "            MRStep(reducer=self.reducer_find_top_words)\n",
        "        ]\n",
        "\n",
        "    def mapper_get_words(self, _, line):\n",
        "        words = WORD_REGEX.findall(line)\n",
        "        for word in words:\n",
        "            yield word.lower(), 1\n",
        "\n",
        "    def combiner_count_words(self, word, counts):\n",
        "        yield word, sum(counts)\n",
        "\n",
        "    def reducer_count_words(self, word, counts):\n",
        "        yield None, (sum(counts), word)\n",
        "\n",
        "    def reducer_find_top_words(self, _, word_count_pairs):\n",
        "        top_n = self.options.top\n",
        "        top_words = sorted(word_count_pairs, reverse=True)[:top_n]\n",
        "        for count, word in top_words:\n",
        "            yield word, count\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TopNWords.run()"
      ],
      "metadata": {
        "id": "PMXl46y91SVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
      ],
      "metadata": {
        "id": "FmJsVkzjwQLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Hadoop cluster information\n",
        "namenode_url = 'http://namenode:50070'\n",
        "datanode_urls = ['http://datanode1:50075', 'http://datanode2:50075']  # Add more datanode URLs if necessary\n",
        "\n",
        "def check_namenode_health():\n",
        "    # Send a GET request to the Namenode's health endpoint\n",
        "    response = requests.get(f'{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus')\n",
        "\n",
        "    # Check the response status code\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        # Get the health status from the response JSON\n",
        "        health_status = data['beans'][0]['State']\n",
        "        print(\"Namenode health status:\", health_status)\n",
        "    else:\n",
        "        print(\"Failed to retrieve Namenode health status.\")\n",
        "\n",
        "def check_datanode_health():\n",
        "    for datanode_url in datanode_urls:\n",
        "        # Send a GET request to the Datanode's health endpoint\n",
        "        response = requests.get(f'{datanode_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo')\n",
        "\n",
        "        # Check the response status code\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            # Get the health status from the response JSON\n",
        "            health_status = data['beans'][0]['DatanodeHealth']\n",
        "            print(f\"Datanode health status ({datanode_url}):\", health_status)\n",
        "        else:\n",
        "            print(f\"Failed to retrieve Datanode health status ({datanode_url}).\")\n",
        "\n",
        "# Check the health status of the NameNode and DataNodes\n",
        "check_namenode_health()\n",
        "check_datanode_health()\n"
      ],
      "metadata": {
        "id": "Co5zNL25wP8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uhK9f0FVwPtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def list_hdfs_path(hdfs_url, path):\n",
        "    # Create an HDFS client\n",
        "    client = InsecureClient(hdfs_url)\n",
        "\n",
        "    # List all files and directories in the given path\n",
        "    files = client.list(path, status=True)\n",
        "\n",
        "    # Print the files and directories\n",
        "    print(\"Files and Directories in\", path)\n",
        "    for file in files:\n",
        "        print(file['path'])\n",
        "\n",
        "# Provide the HDFS URL and path to list\n",
        "hdfs_url = 'http://localhost:50070'\n",
        "path = '/path/to/hdfs/directory'\n",
        "\n",
        "# Call the function to list the files and directories\n",
        "list_hdfs_path(hdfs_url, path)\n"
      ],
      "metadata": {
        "id": "PyGOto1FwPeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n"
      ],
      "metadata": {
        "id": "lTSPqdtewPFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Hadoop cluster information\n",
        "namenode_url = 'http://namenode:50070'\n",
        "\n",
        "def analyze_storage_utilization():\n",
        "    # Send a GET request to the DataNodes endpoint to retrieve information about the DataNodes\n",
        "    response = requests.get(f'{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=DataNodeInfo')\n",
        "\n",
        "    # Check the response status code\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        datanodes = data['beans']\n",
        "\n",
        "        # Create a dictionary to store the storage capacities of DataNodes\n",
        "        storage_capacities = {}\n",
        "\n",
        "        # Iterate over the DataNodes and extract their storage capacities\n",
        "        for datanode in datanodes:\n",
        "            node_name = datanode['DatanodeHostName']\n",
        "            capacity = datanode['Capacity']\n",
        "            storage_capacities[node_name] = capacity\n",
        "\n",
        "        # Find the DataNode with the highest storage capacity\n",
        "        highest_capacity_node = max(storage_capacities, key=storage_capacities.get)\n",
        "\n",
        "        # Find the DataNode with the lowest storage capacity\n",
        "        lowest_capacity_node = min(storage_capacities, key=storage_capacities.get)\n",
        "\n",
        "        # Print the results\n",
        "        print(\"DataNode with the highest storage capacity:\", highest_capacity_node)\n",
        "        print(\"DataNode with the lowest storage capacity:\", lowest_capacity_node)\n",
        "    else:\n",
        "        print(\"Failed to retrieve DataNode information.\")\n",
        "\n",
        "# Analyze the storage utilization of DataNodes\n",
        "analyze_storage_utilization()\n"
      ],
      "metadata": {
        "id": "zEOyE9b8wO1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n"
      ],
      "metadata": {
        "id": "Q7rnvfmCwOlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "# YARN ResourceManager information\n",
        "resourcemanager_url = 'http://resourcemanager:8088'\n",
        "application_id = None\n",
        "\n",
        "# Submit a Hadoop job\n",
        "def submit_hadoop_job():\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    # Specify the Hadoop job details\n",
        "    job_payload = {\n",
        "        \"application\": {\n",
        "            \"applicationName\": \"MyHadoopJob\",\n",
        "            \"amResource\": {\n",
        "                \"vCores\": 1,\n",
        "                \"memory\": 1024\n",
        "            },\n",
        "            \"resource\": {\n",
        "                \"vCores\": 1,\n",
        "                \"memory\": 1024\n",
        "            },\n",
        "            \"priority\": 0,\n",
        "            \"queue\": \"default\",\n",
        "            \"unmanagedAM\": False,\n",
        "            \"keepContainersAcrossApplicationAttempts\": False,\n",
        "            \"maxAppAttempts\": 2,\n",
        "            \"applicationType\": \"MAPREDUCE\",\n",
        "            \"applicationTags\": \"\"\n",
        "        },\n",
        "        \"amContainerSpec\": {\n",
        "            \"commands\": {\n",
        "                \"command\": \"hadoop jar myjob.jar input.txt output\"\n",
        "            }\n",
        "        },\n",
        "        \"applicationTimeouts\": {\n",
        "            \"timeout\": 0\n",
        "        },\n",
        "        \"attemptFailuresValidityInterval\": -1,\n",
        "        \"logAggregationContext\": {\n",
        "            \"logAggregationEnabled\": False,\n",
        "            \"logAggregationContext\": {\n",
        "                \"logIncludePattern\": \"\",\n",
        "                \"logExcludePattern\": \"\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Send a POST request to submit the Hadoop job\n",
        "    response = requests.post(f'{resourcemanager_url}/ws/v1/cluster/apps', json=job_payload, headers=headers)\n",
        "\n",
        "    # Check the response status code\n",
        "    if response.status_code == 202:\n",
        "        data = response.json()\n",
        "        global application_id\n",
        "        application_id = data['application-id']\n",
        "        print(\"Hadoop job submitted successfully. Application ID:\", application_id)\n",
        "    else:\n",
        "        print(\"Failed to submit the Hadoop job.\")\n",
        "\n",
        "# Monitor job progress and retrieve final output\n",
        "def monitor_job_progress():\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    while True:\n",
        "        # Send a GET request to retrieve job status\n",
        "        response = requests.get(f'{resourcemanager_url}/ws/v1/cluster/apps/{application_id}', headers=headers)\n",
        "\n",
        "        # Check the response status code\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            status = data['app']['finalStatus']\n",
        "            if status == 'SUCCEEDED':\n",
        "                print(\"Hadoop job completed successfully.\")\n",
        "                break\n",
        "            elif status == 'FAILED':\n",
        "                print(\"Hadoop job failed.\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"Job is still running...\")\n",
        "        else:\n",
        "            print(\"Failed to retrieve job status.\")\n",
        "            break\n",
        "\n",
        "        # Wait for 5 seconds before checking the job status again\n",
        "        time.sleep(5)\n",
        "\n",
        "    # Retrieve the final output\n",
        "    if status == 'SUCCEEDED':\n",
        "        # Modify the output URL as per your Hadoop configuration\n",
        "        output_url = f'{resourcemanager_url}/proxy/{application_id}/ws/v1/mapreduce/jobs/{application_id}/jobattempts'\n",
        "        response = requests.get(output_url)\n",
        "\n",
        "        # Check the response status code\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(\"Final output:\")\n",
        "            print(data)\n",
        "        else:\n",
        "            print(\"Failed to retrieve the final output.\")\n",
        "\n",
        "# Submit the Hadoop job\n",
        "submit_hadoop_job()\n",
        "\n",
        "# Monitor the job progress and retrieve the final output\n",
        "if application_id:\n",
        "    monitor_job_progress()\n"
      ],
      "metadata": {
        "id": "1EFxSCOTwOVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n"
      ],
      "metadata": {
        "id": "SHykmEe-wOAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "# YARN ResourceManager information\n",
        "resourcemanager_url = 'http://resourcemanager:8088'\n",
        "application_id = None\n",
        "\n",
        "# Submit a Hadoop job with resource requirements\n",
        "def submit_hadoop_job():\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    # Specify the Hadoop job details\n",
        "    job_payload = {\n",
        "        \"application\": {\n",
        "            \"applicationName\": \"MyHadoopJob\",\n",
        "            \"amResource\": {\n",
        "                \"vCores\": 1,\n",
        "                \"memory\": 1024\n",
        "            },\n",
        "            \"resource\": {\n",
        "                \"vCores\": 2,\n",
        "                \"memory\": 2048\n",
        "            },\n",
        "            \"priority\": 0,\n",
        "            \"queue\": \"default\",\n",
        "            \"unmanagedAM\": False,\n",
        "            \"keepContainersAcrossApplicationAttempts\": False,\n",
        "            \"maxAppAttempts\": 2,\n",
        "            \"applicationType\": \"MAPREDUCE\",\n",
        "            \"applicationTags\": \"\"\n",
        "        },\n",
        "        \"amContainerSpec\": {\n",
        "            \"commands\": {\n",
        "                \"command\": \"hadoop jar myjob.jar input.txt output\"\n",
        "            }\n",
        "        },\n",
        "        \"applicationTimeouts\": {\n",
        "            \"timeout\": 0\n",
        "        },\n",
        "        \"attemptFailuresValidityInterval\": -1,\n",
        "        \"logAggregationContext\": {\n",
        "            \"logAggregationEnabled\": False,\n",
        "            \"logAggregationContext\": {\n",
        "                \"logIncludePattern\": \"\",\n",
        "                \"logExcludePattern\": \"\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Send a POST request to submit the Hadoop job\n",
        "    response = requests.post(f'{resourcemanager_url}/ws/v1/cluster/apps', json=job_payload, headers=headers)\n",
        "\n",
        "    # Check the response status code\n",
        "    if response.status_code == 202:\n",
        "        data = response.json()\n",
        "        global application_id\n",
        "        application_id = data['application-id']\n",
        "        print(\"Hadoop job submitted successfully. Application ID:\", application_id)\n",
        "    else:\n",
        "        print(\"Failed to submit the Hadoop job.\")\n",
        "\n",
        "# Track resource usage during job execution\n",
        "def track_resource_usage():\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    while True:\n",
        "        # Send a GET request to retrieve resource usage\n",
        "        response = requests.get(f'{resourcemanager_url}/ws/v1/cluster/apps/{application_id}/appattempts', headers=headers)\n",
        "\n",
        "        # Check the response status code\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            attempts = data['appAttempts']\n",
        "            if attempts:\n",
        "                # Get the latest attempt ID\n",
        "                latest_attempt_id = attempts[-1]['appAttemptId']\n",
        "\n",
        "                # Send a GET request to retrieve the attempt details\n",
        "                response = requests.get(f'{resourcemanager_url}/ws/v1/cluster/apps/{application_id}/appattempts/{latest_attempt_id}', headers=headers)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    resource_usage = data['appAttempt']['allocatedResources']\n",
        "                    print(\"Resource usage:\", resource_usage)\n",
        "                else:\n",
        "                    print(\"Failed to retrieve resource usage.\")\n",
        "                    break\n",
        "            else:\n",
        "                print(\"Job attempt not found.\")\n",
        "                break\n",
        "        else:\n",
        "            print(\"Failed to retrieve job attempts.\")\n",
        "            break\n",
        "\n",
        "        # Wait for 5 seconds before checking resource usage again\n",
        "        time.sleep(5)\n",
        "\n",
        "# Submit the Hadoop job\n",
        "submit_hadoop_job()\n",
        "\n",
        "# Track resource usage during job execution\n",
        "if application_id:\n",
        "    track_resource_usage()\n"
      ],
      "metadata": {
        "id": "aAoQl_9vwqdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
      ],
      "metadata": {
        "id": "-CFdxA1owrJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import time\n",
        "\n",
        "# Define the MapReduce job\n",
        "class MapReduceJob(MRJob):\n",
        "\n",
        "    def configure_args(self):\n",
        "        super(MapReduceJob, self).configure_args()\n",
        "        self.add_passthru_arg('--split-size', type=int, help='Input split size in bytes')\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        yield line.split()[0], 1\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        yield key, sum(values)\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper, reducer=self.reducer)\n",
        "        ]\n",
        "\n",
        "# Test the MapReduce job with different input split sizes\n",
        "def compare_job_performance():\n",
        "    input_data = [\n",
        "        \"apple orange\",\n",
        "        \"banana apple\",\n",
        "        \"orange apple\",\n",
        "        \"banana orange\",\n",
        "        \"apple banana\",\n",
        "        \"orange banana\"\n",
        "    ]\n",
        "\n",
        "    # Varying input split sizes to test\n",
        "    split_sizes = [100, 200, 300]\n",
        "\n",
        "    for split_size in split_sizes:\n",
        "        # Create an instance of the MapReduce job\n",
        "        mr_job = MapReduceJob(args=['--split-size', str(split_size)])\n",
        "\n",
        "        # Start the timer\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run the MapReduce job\n",
        "        with mr_job.make_runner() as runner:\n",
        "            runner.run()\n",
        "\n",
        "            # Collect and print the output\n",
        "            output = [line.strip() for line in runner.cat_output()]\n",
        "            print(\"Output for split size:\", split_size)\n",
        "            for line in output:\n",
        "                print(line)\n",
        "\n",
        "        # Calculate and print the execution time\n",
        "        execution_time = time.time() - start_time\n",
        "        print(\"Execution time for split size\", split_size, \":\", execution_time, \"seconds\")\n",
        "        print()\n",
        "\n",
        "# Compare the performance of the MapReduce job\n",
        "compare_job_performance()\n"
      ],
      "metadata": {
        "id": "6u06M5Z23QR6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}